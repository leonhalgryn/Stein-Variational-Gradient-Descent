{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background on Kernels and Reproducing Kernel Hilbert Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an overview and recapitulation of kernels and reproducing kernel Hilbert spaces (RKHSs). The majority of this notebook is taken directly from \\[[1](#References)\\] and \\[[2](#References)\\].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation and Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wish to fit a simple linear regression model to a set of data $\\mathcal{D} = \\{(x_i, y_i): i=1,2,\\dots,N\\}$, where $x_i \\in \\mathbb{R}$ denotes the observations of the predictors, and $y_i \\in \\mathbb{R}$ denotes the observed response variable. Furthermore, suppose that we observe that the relationship between $x_i$ and $y_i$ is non-linear. In this case, a simple linear model $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$ would yield unsatisfactory predictive performance. We can improve the model performance by increasing the model complexity/capacity. One approach to do this is to consider enlarging the original input space using a feature map $\\phi: \\mathbb{R} \\rightarrow \\mathbb{R}^d$ defined by:\n",
    "$$x_i \\mapsto \\begin{bmatrix} x_i & x_i^2 & x_i^3 & \\dots & x_i^d\\end{bmatrix}^T$$\n",
    " and then fitting a linear regression model in the enlarged feature space: $\\hat{y}_i = \\hat{\\alpha}_0 + \\hat{\\pmb{\\alpha}}^T \\phi(x_i)$<br>\n",
    "In this example, the fitted line will be linear in the enlarged feature space, but will be non-linear in the original input space, allowing for a much better fit to the data.\n",
    "However, the challenge is that constructing the feature map $\\phi: \\mathbb{R} \\rightarrow \\mathbb{R}^d$ demands substantial domain knowledge. To address this issue, one can turn to a kernel function. A kernel function facilitates the computation of inner products between features after they have been transformed into a higher-dimensional (often infinite-dimensional) space. This is achieved without requiring explicit specification or computation of the feature maps. That is, a kernel function $k(x, x')$ computes the inner product between transformed features $\\phi(x)$ and $\\phi(x')$, i.e. \n",
    "$$k(x, x') = \\langle \\phi(x), \\phi(x') \\rangle$$\n",
    "\n",
    "It is helpful to think of a kernel function, $k(x, x')$, as providing a measure of similarity between the feature maps $\\phi(x)$ and $\\phi(x')$ corresponding to input points $x$ and $x'$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hilbert Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Definition 1.1}}$: (Vector space)\n",
    "\n",
    "A non-empty set $V$ on which addition and scalar multiplication is defined is a vector space if the following axioms are satisfied for all $\\pmb{u}, \\pmb{v}, \\pmb{w} \\in V$ and for all $a, b \\in \\mathbb{R}$:\n",
    "\n",
    "1. $\\pmb{u} + \\pmb{v} \\in V$\n",
    "2. $\\pmb{u} + \\pmb{v} = \\pmb{v} + \\pmb{u}$\n",
    "3. $(\\pmb{u} + \\pmb{v}) + \\pmb{w} = \\pmb{u} + (\\pmb{v} + \\pmb{w})$\n",
    "4. There exists a vector (called the zero vector) $\\pmb{0} \\in V$ such that $\\pmb{u} + \\pmb{0} = \\pmb{u}$\n",
    "5. $\\forall \\pmb{u} \\in V \\exists (-\\pmb{u}) \\in V$ such that $\\pmb{u} + (- \\pmb{u}) = \\pmb{0}$\n",
    "6. $a \\pmb{u} \\in V$\n",
    "7. $a(\\pmb{u} + \\pmb{v}) = a \\pmb{u} + a \\pmb{v}$\n",
    "8. $(a + b)\\pmb{u} = a \\pmb{u} + b \\pmb{u}$\n",
    "9. $(ab)\\pmb{u} = a(b \\pmb{u})$ \n",
    "10. $1 \\pmb{u} = \\pmb{u}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Definition 1.2}}$: (Normed space)\n",
    "\n",
    "A normed space is a pair $(V, ||\\cdot||)$, where $V$ is a vector space and $||\\cdot||:V \\rightarrow \\mathbb{R}$ is a function (called a norm) with the following properties:\n",
    "\n",
    "1. $||\\pmb{v}|| \\ge 0 \\hspace{0.1cm}\\forall \\pmb{v} \\in V$\n",
    "2. $||\\pmb{v}|| = 0$ if and only if $\\pmb{v} = \\pmb{0}$\n",
    "3. $||a \\pmb{v}|| = |a|\\cdot ||\\pmb{v}|| \\hspace{0.1cm} \\forall \\pmb{v} \\in V, \\forall a \\in \\mathbb{R}$\n",
    "4. Triangle inequality: $||\\pmb{u} + \\pmb{v}|| \\le ||\\pmb{u}|| + ||\\pmb{v}|| \\hspace{0.1cm} \\forall \\pmb{u}, \\pmb{v} \\in V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Definition 1.3}}$: (Metric space)\n",
    "\n",
    "A metric space is a pair $(V, d)$ where $V$ is a vector space and $d:V \\times V \\rightarrow \\mathbb{R}$ is a function (called a metric) with the following properties:\n",
    "\n",
    "1. $d(v, w) \\ge 0 \\hspace{0.1cm}\\forall v,w \\in V$\n",
    "2. $d(v, w) = 0$ if and only if $v = w$\n",
    "3. $d(v, w) = d(w, v)$\n",
    "4. Triangle inequality: $d(v, w) \\le d(v, u) + d(u, w) \\hspace{0.1cm}\\forall u,v,w \\in V$\n",
    "\n",
    "$\\textbf{Remark}$: A metric generalises the notion of distance to non-Euclidean spaces.\n",
    "\n",
    "$\\textbf{Remark}$: Every norm $||\\cdot||$ induces a metric given by $d(v, w) = ||v - w||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Definition 1.4}}$: (Cauchy sequence and completeness)\n",
    "\n",
    "1. A sequence $(x_n)_{n=1}^\\infty$ in a metric space $(V, d)$ is called a Cauchy sequence if and only if $\\forall \\epsilon \\gt 0 \\exists N \\in \\mathbb{N}$ such that $\\forall n,m \\ge N$ we have that $d(x_n, x_m) \\lt \\epsilon$.\n",
    "2. A metric space $(V, d)$ is complete if every Cauchy sequence in $(V, d)$ converges to an element in $V$.\n",
    "3. A complete normed space is called a Banach space.\n",
    "\n",
    "$\\textbf{Remark}$: Essentially, a Cauchy sequence is one in which the elements of the sequence progressively become closer and closer to one another. That is, the terms in a Cauchy sequence become arbitrarily close as the sequence progresses. Note that, while all Cauchy sequences are bounded, not all Cauchy sequences are convergent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Definition 1.5}}$: (Inner product space)\n",
    "\n",
    "An inner product space is a pair $(V, \\langle\\cdot, \\cdot \\rangle)$ where $V$ is a vector space over $\\mathbb{R}$ and $\\langle\\cdot, \\cdot \\rangle$ is an inner product on $V$. That is, $\\langle\\cdot, \\cdot \\rangle: V \\times V \\rightarrow \\mathbb{R}$ is a function satisfying the following properties:\n",
    "\n",
    "1. $\\langle u, v \\rangle = \\langle v, u \\rangle \\hspace{0.1cm}\\forall u,v \\in V$\n",
    "2. $\\langle u, u \\rangle \\ge 0 \\hspace{0.1cm}\\forall u \\in V$\n",
    "3. $\\langle u, u \\rangle = 0$ if and only if $u = 0$\n",
    "4. $\\langle u, v + w \\rangle = \\langle u, v \\rangle + \\langle u, w \\rangle \\hspace{0.1cm}\\forall u,v,w \\in V$\n",
    "5. $\\langle \\alpha u, v\\rangle = \\alpha \\langle u, v \\rangle \\hspace{0.1cm}\\forall u,v \\in V, \\forall \\alpha \\in \\mathbb{R}$\n",
    "\n",
    "$\\textbf{Remark}$: Every inner product $\\langle \\cdot, \\cdot \\rangle$ induces a norm given by $||\\cdot|| = \\sqrt{\\langle \\cdot, \\cdot\\rangle}$, which in turn induces a metric as stated in a previous remark. Hence, an inner product also induces a metric $d(v, w) = \\sqrt{\\langle v - w, v - w\\rangle}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Definition 1.6}}$ (Hilbert space)\n",
    "\n",
    "An inner product space $(V, \\langle \\cdot, \\cdot \\rangle)$ which is complete (with respect to the metric induced by the inner product) is said to be a Hilbert space. That is, a Hilbert space is an inner product space in which all Cauchy sequences converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kernels and RKHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is based entirely on \\[[1](#References)\\]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Definition 1.7}}$: (Kernel)\n",
    "\n",
    "Let $\\mathcal{X}$ be a non-empty set. A function $k:\\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ is called a kernel if there exists a $\\mathbb{R}$-Hilbert space $\\mathcal{H}$ and a map $\\phi: \\mathcal{X} \\rightarrow \\mathcal{H}$ such that the following holds $\\forall x,x' \\in \\mathcal{X}$:\n",
    "\n",
    "$$k(x, x') = \\langle \\phi(x), \\phi(x') \\rangle_{\\mathcal{H}}$$\n",
    "\n",
    "$\\textbf{Remark}$: The map $\\phi$ in Definition 1.7 is often referred to as a feature map. It is common for a feature map to transform the space into a higher (possibly infinite) dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Definition 1.8}}$: (Positive definite and semi-definite)\n",
    "\n",
    "A symmetric function $k: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$, with $\\mathcal{X}$ a non-empty set is said to be positive semi-definite if and only if $\\forall n \\ge 1$, $\\forall a_1, a_2, \\dots, a_n \\in \\mathbb{R}$ $\\forall x_1, x_2, \\dots, x_n \\in \\mathcal{X}$ we have that:\n",
    "\n",
    "$$\\sum_{i=1}^n \\sum_{j=1}^n a_i a_j k(x_i, x_j) \\ge 0$$\n",
    "\n",
    "If the above quantity is strictly greater than zero, then the function is said to be positive definite (sometimes referred to as strictly positive definite)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Proposition 1.9}}$: (Kernels are positive semi-definite)\n",
    "\n",
    "Let $\\mathcal{H}$ be a Hilbert space, $\\mathcal{X}$ a non-empty set and $\\phi: \\mathcal{X} \\rightarrow \\mathcal{H}$ a map. Then the kernel $k(x, x') = \\langle \\phi(x), \\phi(x') \\rangle_{\\mathcal{H}}$ is positive semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use kernel functions to define functions on $\\mathcal{X}$, giving rise to a space of functions known as a reproducing kernel Hilbert space (RKHS).\n",
    "\n",
    "Before we give a formal definition of a RKHS, we will first provide a motivating example taken from \\[[1](#References)\\].\n",
    "\n",
    "Consider the feature map $\\phi: \\mathcal{X} \\rightarrow \\mathbb{R}^3$ with $\\mathcal{X} = \\mathbb{R}^2$, defined by:\n",
    "\n",
    "$$x = \\begin{bmatrix}x_1 \\\\ x_2 \\end{bmatrix} \\mapsto \\phi(x) = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_1 x_2 \\end{bmatrix}$$\n",
    "\n",
    "with the kernel function given by the standard Euclidean inner product (dot product):\n",
    "\n",
    "$$k(x, y) := \\langle \\phi(x), \\phi(y) \\rangle = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_1 x_2 \\end{bmatrix}^T \\begin{bmatrix}y_1 \\\\ y_2 \\\\ y_1 y_2 \\end{bmatrix}$$\n",
    "\n",
    "Let $\\mathcal{H} = \\{\\phi(x) : x \\in \\mathcal{X}\\}$ denote the feature space associated with the feature map $\\phi$. We can now define a function of the features $\\phi(x)$ of $x$ as:\n",
    "\n",
    "$$f(x) = \\alpha_1 x_1 + \\alpha_2 x_2 + \\alpha_3 x_1 x_2$$\n",
    "\n",
    "We can define an equivalent representation of the function as:\n",
    "\n",
    "$$f(\\cdot) = \\begin{bmatrix}\\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{bmatrix}$$\n",
    "\n",
    "Here, $f(x) \\in \\mathbb{R}$ denotes the function evaluation at a point $x$, and $f(\\cdot) \\equiv f$ denotes the function itself. Using this notation, we can rewrite the function evaluation as:\n",
    "\n",
    "$$f(x) = f(\\cdot)^T \\phi(x) = \\langle f(\\cdot), \\phi(x) \\rangle_{\\mathcal{H}}$$\n",
    "\n",
    "This means that the function $f(\\cdot)$ can be evaluated at a point $x$ by taking the inner product (in feature space) of the function $f(\\cdot)$ and the feature map $\\phi(x)$.\n",
    "\n",
    "Using this same line of reasoning, we can write:\n",
    "\n",
    "$$k(\\cdot, y) = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ y_1 y_2 \\end{bmatrix} = \\phi(y)$$\n",
    "\n",
    "which then allows us to rewrite:\n",
    "\n",
    "$$\\langle k(\\cdot, y), \\phi(x) \\rangle_{\\mathcal{H}} = \\alpha_1 x_1 + \\alpha_2 x_2 + \\alpha_3 x_1 x_2$$\n",
    "\n",
    "$$= \\langle \\phi(y), \\phi(x) \\rangle_{\\mathcal{H}} = k(y, x)$$\n",
    "\n",
    "where $\\alpha_1 = y_1, \\alpha_2 = y_2, \\alpha_3 = y_1 y_2$. <br>\n",
    "This shows that $\\phi$ can be interpreted as a mapping from $\\mathcal{X} \\rightarrow \\mathbb{R}^3$, but that it can also be interpreted as defining the parameters of a function mapping from $\\mathbb{R}^2 \\rightarrow \\mathbb{R}$.\n",
    "This then also yields the so-called canonical feature map representation of the feature map $\\phi$, given by $\\phi(x) = k(\\cdot, x)$ and $\\phi(y) = k(\\cdot, y)$.\n",
    "\n",
    "This example illustrates the two defining properties of a RKHS, namely:\n",
    "\n",
    "1. The feature map of each point is in the feature space. That is,\n",
    "\n",
    "$$k(\\cdot, x) = \\phi(x) \\in \\mathcal{H} \\hspace{0.1cm}\\forall x \\in \\mathcal{X}$$\n",
    "\n",
    "2. The reproducing property:\n",
    "\n",
    "$$\\langle f, k(\\cdot, x) \\rangle_{\\mathcal{H}} = f(x)\\hspace{0.1cm} \\forall x \\in \\mathcal{X}, \\forall f \\in \\mathcal{H}$$\n",
    "\n",
    "These two properties also allow us to rewrite:\n",
    "\n",
    "$$k(x, y) = \\langle \\phi(x), \\phi(y)\\rangle_{\\mathcal{H}} = \\langle k(\\cdot, x), k(\\cdot, y) \\rangle_{\\mathcal{H}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now give the formal definition of a RKHS.\n",
    "\n",
    "$\\underline{\\textbf{Definition 1.10}}$: (RKHS)\n",
    "\n",
    "Let $\\mathcal{H}$ be a Hilbert space consisting of functions on a non-empty set $\\mathcal{X}$. A function $k: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ is called a $\\textit{reproducing kernel}$ of $\\mathcal{H}$, and $\\mathcal{H}$ called a reproducing kernel Hilbert space (RKHS) if $k$ satisfies the following properties:\n",
    "\n",
    "1. $k(\\cdot, x) \\in \\mathcal{H} \\hspace{0.1cm}\\forall x \\in \\mathcal{X}$\n",
    "2. $\\forall x \\in \\mathcal{X}, \\forall f \\in \\mathcal{H}$ we have that $\\langle f, k(\\cdot, x) \\rangle_{\\mathcal{H}} = f(x)$ $\\hspace{0.2cm}$(Reproducing property)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[1\\] Gretton, A. (2019) 'Introduction to RKHS, and some simple kernel algorithms', Lecture 4, University College London, 16 October 2019. Available at: https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf \n",
    "\n",
    "\\[2\\] Mehta, B. (2022) 'Stein Variational Gradient Descent', Depth First Learning, 2 March 2022. Available at: https://www.depthfirstlearning.com/2020/SVGD.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
